# Loan Amount Prediction using Machine Learning

## ğŸ“Œ Assignment Overview
This Assignment focuses on predicting **Loan Sanction Amounts** using **Linear Regression** and other regression models. The work was completed as part of the **Machine Learning Algorithms Laboratory** (ICS1512) at **Sri Sivasubramaniya Nadar College of Engineering, Chennai**.

The assignment covers the **end-to-end machine learning workflow**, including data preprocessing, exploratory data analysis, feature engineering, model training, evaluation, and visualization.

---

## ğŸ¯ Aim
To build and evaluate regression models for **loan sanction prediction**, incorporating:
- **K-Fold Cross Validation**
- **Performance metrics (MAE, MSE, RMSE, RÂ²)**
- **Visualization techniques (Residual plots, Correlation Heatmaps, Actual vs Predicted plots, etc.)**

---

## ğŸ› ï¸ Libraries Used
- **pandas** â€“ Data manipulation and analysis  
- **numpy** â€“ Numerical computations  
- **matplotlib** â€“ Data visualization  
- **seaborn** â€“ Statistical data visualization  
- **scikit-learn** â€“ Model training, validation, and evaluation  

---

## ğŸ“‚ Dataset
- Original dataset size: **30,000 rows Ã— 24 columns**  
- After preprocessing (handling missing values & outliers): **29,660 rows Ã— 24 columns**  

### Features
- Demographic features: *Gender, Age, Income, Income Stability*  
- Financial features: *Credit Score, Loan Amount Request, Property Price*  
- Loan history: *No. of Defaults, Active Credit Card, Dependents*  
- Property details: *Property Age, Property Type, Property Location*  

### Target
- **Loan Sanction Amount (USD)**

---

## ğŸ”„ Data Preprocessing
1. **Handling Missing Values** â€“ filled using mean/median/mode.  
2. **Outlier Treatment** â€“ capped using percentile-based approach.  
3. **Encoding Categorical Variables** â€“ Label Encoding applied.  
4. **Feature Scaling** â€“ StandardScaler used for normalization.  

---

## ğŸ“Š Exploratory Data Analysis
- **Histograms & Distribution plots** to check feature distributions.  
- **Scatter plots** for Income, Credit Score vs Loan Amount.  
- **Correlation Heatmap** to study feature relationships.  
- **Boxplots** to identify and treat outliers.  

---

## ğŸ§  Models Implemented
- **Linear Regression (Baseline)**
- **Regularized Regression:** Ridge, Lasso, ElasticNet
- **Polynomial Regression**
- **Decision Tree Regressor**
- **Random Forest Regressor**
- **AdaBoost Regressor**
- **Gradient Boosting Regressor**
- **XGBoost Regressor**
- **Support Vector Regressors (Linear, Polynomial, RBF, Sigmoid)**
- **K-Neighbors Regressor**

---

## ğŸ“ˆ Performance Metrics
Evaluation was performed using:
- **Mean Absolute Error (MAE)**
- **Mean Squared Error (MSE)**
- **Root Mean Squared Error (RMSE)**
- **RÂ² Score**
- **Adjusted RÂ² Score**

### Cross-Validation (K=5)
- Linear Regression Average: **MAE = 18,891.93 | RMSE = 26,895.81 | RÂ² = 0.62**

### Final Test Results (Best Models)
| Model                | MAE     | RMSE     | RÂ²   |
|-----------------------|---------|----------|------|
| **Linear Regression** | 19,022  | 27,266   | 0.60 |
| **Decision Tree**     | 1,211   | 10,977   | 0.94 |
| **Random Forest**     | **928** | **9,599**| 0.95 |
| **Gradient Boosting** | 951     | 9,747    | 0.95 |
| **XGBoost**           | **808** | **8,826**| **0.96** |

---

## ğŸ“Š Visualizations
- Histogram / Distribution plots  
- Scatter plots  
- Correlation heatmap  
- Actual vs Predicted plots  
- Residual plots  
- Boxplots  
- Feature Coefficient bar plots  

---

## âœ… Key Learnings
- End-to-end workflow of a regression task: **Preprocessing â†’ Training â†’ Evaluation**  
- Importance of **data cleaning** (missing values, outliers)  
- How **cross-validation** strengthens model reliability  
- Performance trade-offs between **linear models and ensemble models**  
- Visual validation using **residuals and actual vs predicted plots**  

---

## ğŸš€ Conclusion
- Linear models provided a strong baseline, but ensemble methods like **Random Forest, Gradient Boosting, and XGBoost** significantly outperformed them.  
- **XGBoost Regressor** achieved the **best performance** with **MAE â‰ˆ 808, RMSE â‰ˆ 8,826, and RÂ² â‰ˆ 0.96**.  
- This shows ensemble learning techniques are more suitable for complex, non-linear datasets like loan prediction.  

---

## ğŸ“ Author
- **Student Name:** Anusha K N 
- **Batch:** 2023â€“2028  
- **Course:** Machine Learning Algorithms Lab (ICS1512)  

---
